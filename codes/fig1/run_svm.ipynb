{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f43a57-8dde-48d7-9956-991c55b335fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "sys.path.append('../')\n",
    "\n",
    "from packages import actv_analysis, svm\n",
    "import seaborn as sns\n",
    "# from packages.svm import SVM_fit\n",
    "# from packages.load_csv import units_for_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a72382-455f-4e29-8f17-5bd630e53954",
   "metadata": {},
   "source": [
    "## Make training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd633cc2-a5de-404c-8f8e-4a101ff4a959",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nidx=range(1,10)\n",
    "sidx=range(3,10)\n",
    "img_inst=500\n",
    "\n",
    "for i in range(10):\n",
    "    rep_per_ns_combo=16\n",
    "    df_idx = svm.gen_SVM_input(nidx=nidx, sidx=sidx, rep_per_ns_combo=rep_per_ns_combo, img_inst=img_inst)\n",
    "    # save to csv file\n",
    "    df_idx.to_csv(f'svm_training_set{i}_4to20.csv', index=True)\n",
    "    \n",
    "    rep_per_ns_combo=4\n",
    "    df_idx = svm.gen_SVM_input(nidx=nidx, sidx=sidx, rep_per_ns_combo=rep_per_ns_combo, img_inst=img_inst)\n",
    "    # save to csv file\n",
    "    df_idx.to_csv(f'svm_test_set{i}_4to20.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9150d56a-9bc8-463d-920e-aed97a59ab5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Parameters\n",
    "#relus = range(2,6)\n",
    "relus = list(range(4,5))\n",
    "epochs = np.arange(0, 91, 10)\n",
    "exps = np.arange(0, 10)\n",
    "num_units = 100\n",
    "rate_threshold = 0.05\n",
    "layer_numunits = {'relu1':290400, 'relu2':186624, 'relu3':64896, 'relu4':64896, 'relu5':43264}\n",
    "\n",
    "dir_path = os.path.dirname(os.path.realpath('../'))\n",
    "#path_for_units = f\"{dir_path}/dataframes/SVM/units/{num_units} units sampled from distribution higher than {rate_threshold} response rate including PN2 and PN20\"\n",
    "set_folder = f\"{dir_path}/dataframes/SVM\"  # retrieve training/test sets\n",
    "save_to_folder = f\"{dir_path}/dataframes/SVM_predictions\"\n",
    "\n",
    "for relu in relus:\n",
    "    for epoch in epochs:\n",
    "        for net in range(1, 2):\n",
    "            pkl_filename = f'pkl/network{net}_Relu{relu}_epoch{epoch}.pkl'\n",
    "            print(f'Loading {pkl_filename}..')\n",
    "            with open(pkl_filename, 'rb') as f:\n",
    "                units = pickle.load(f)\n",
    "            print(f'Loading actv ..')\n",
    "            actv_net = actv_analysis.get_actv_net(net=net, relu=relu, epoch=epoch)\n",
    "            actv = actv_net.reshape(list(layer_numunits.values())[relu-1], 10, 10, 500)\n",
    "            \n",
    "            # select units whose activities corresponding to images will be used for svm training and testing:\n",
    "            units_with_someresp = [units[i].id for i in range(len(units)) if units[i].response_rate_subset > rate_threshold]\n",
    "            \n",
    "            # Randomly choose \"num_units\" number of units from units_with_someresp without replacement\n",
    "            random.seed(exp)\n",
    "            units = random.sample(units_with_someresp, num_units)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            y_preds = Parallel(n_jobs=-1)(delayed(svm.SVM_fit)(units=units, actv=actv, exp=exp) for exp in exps)\n",
    "            end_time = time.time()\n",
    "            print(f\"Took {end_time - start_time} seconds to run.\")\n",
    "\n",
    "            [pd.Series(y_preds[exp]).to_csv(f'csv/SVM prediction of He untrained net{net} relu{relu} epoch{epoch} {num_units} units that are randomly drawn from distribution exp{exp} June2023.csv', index=True) for exp in exps]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79223798-f360-4899-896f-28eb49708f47",
   "metadata": {},
   "source": [
    "## Run SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cfb8f70-7498-4986-8beb-6b3bdffc8ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSV files for net1 relu2 epoch60 already exist, skipping.\n",
      "All CSV files for net2 relu2 epoch60 already exist, skipping.\n",
      "Loading pkl/4to20/network3_Relu2_epoch60_4to20.pkl and actv file..\n",
      "--- 317.62293910980225 seconds ---\n",
      "Took 84.94265007972717 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network4_Relu2_epoch60_4to20.pkl and actv file..\n",
      "--- 320.8245918750763 seconds ---\n",
      "Took 83.26117372512817 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network5_Relu2_epoch60_4to20.pkl and actv file..\n",
      "--- 319.86405515670776 seconds ---\n",
      "Took 85.52524471282959 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network6_Relu2_epoch60_4to20.pkl and actv file..\n",
      "--- 323.1744019985199 seconds ---\n",
      "Took 90.97447609901428 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network7_Relu2_epoch60_4to20.pkl and actv file..\n",
      "--- 2102.621785879135 seconds ---\n",
      "Took 559.8140249252319 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network8_Relu2_epoch60_4to20.pkl and actv file..\n",
      "--- 329.4616599082947 seconds ---\n",
      "Took 85.13339328765869 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network9_Relu2_epoch60_4to20.pkl and actv file..\n",
      "--- 334.12975001335144 seconds ---\n",
      "Took 85.8435697555542 seconds to run for num_units 200.\n",
      "All CSV files for net10 relu2 epoch60 already exist, skipping.\n",
      "All CSV files for net1 relu2 epoch70 already exist, skipping.\n",
      "All CSV files for net2 relu2 epoch70 already exist, skipping.\n",
      "Loading pkl/4to20/network3_Relu2_epoch70_4to20.pkl and actv file..\n",
      "--- 341.65318512916565 seconds ---\n",
      "Took 90.89234614372253 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network4_Relu2_epoch70_4to20.pkl and actv file..\n",
      "--- 341.7398171424866 seconds ---\n",
      "Took 90.58287000656128 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network5_Relu2_epoch70_4to20.pkl and actv file..\n",
      "--- 336.0775098800659 seconds ---\n",
      "Took 88.89001274108887 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network6_Relu2_epoch70_4to20.pkl and actv file..\n",
      "--- 342.04263496398926 seconds ---\n",
      "Took 88.86696314811707 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network7_Relu2_epoch70_4to20.pkl and actv file..\n",
      "--- 782.2331869602203 seconds ---\n",
      "Took 94.0380687713623 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network8_Relu2_epoch70_4to20.pkl and actv file..\n",
      "--- 347.8200261592865 seconds ---\n",
      "Took 89.11319208145142 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network9_Relu2_epoch70_4to20.pkl and actv file..\n",
      "--- 339.6834008693695 seconds ---\n",
      "Took 91.97239804267883 seconds to run for num_units 200.\n",
      "All CSV files for net10 relu2 epoch70 already exist, skipping.\n",
      "All CSV files for net1 relu2 epoch80 already exist, skipping.\n",
      "All CSV files for net2 relu2 epoch80 already exist, skipping.\n",
      "Loading pkl/4to20/network3_Relu2_epoch80_4to20.pkl and actv file..\n",
      "--- 344.8663022518158 seconds ---\n",
      "Took 92.39333605766296 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network4_Relu2_epoch80_4to20.pkl and actv file..\n",
      "--- 338.9656548500061 seconds ---\n",
      "Took 88.37038993835449 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network5_Relu2_epoch80_4to20.pkl and actv file..\n",
      "--- 335.28509521484375 seconds ---\n",
      "Took 86.7968361377716 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network6_Relu2_epoch80_4to20.pkl and actv file..\n",
      "--- 337.28988790512085 seconds ---\n",
      "Took 85.02731084823608 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network7_Relu2_epoch80_4to20.pkl and actv file..\n",
      "--- 334.9526951313019 seconds ---\n",
      "Took 87.93191289901733 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network8_Relu2_epoch80_4to20.pkl and actv file..\n",
      "--- 338.7333331108093 seconds ---\n",
      "Took 89.62789797782898 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network9_Relu2_epoch80_4to20.pkl and actv file..\n",
      "--- 341.4175741672516 seconds ---\n",
      "Took 89.31023788452148 seconds to run for num_units 200.\n",
      "All CSV files for net10 relu2 epoch80 already exist, skipping.\n",
      "All CSV files for net1 relu2 epoch90 already exist, skipping.\n",
      "All CSV files for net2 relu2 epoch90 already exist, skipping.\n",
      "Loading pkl/4to20/network3_Relu2_epoch90_4to20.pkl and actv file..\n",
      "--- 346.7863380908966 seconds ---\n",
      "Took 90.57878303527832 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network4_Relu2_epoch90_4to20.pkl and actv file..\n",
      "--- 338.95934104919434 seconds ---\n",
      "Took 86.39338207244873 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network5_Relu2_epoch90_4to20.pkl and actv file..\n",
      "--- 332.3120448589325 seconds ---\n",
      "Took 85.0430998802185 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network6_Relu2_epoch90_4to20.pkl and actv file..\n",
      "--- 336.78066301345825 seconds ---\n",
      "Took 89.08265113830566 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network7_Relu2_epoch90_4to20.pkl and actv file..\n",
      "--- 337.9908127784729 seconds ---\n",
      "Took 88.1357901096344 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network8_Relu2_epoch90_4to20.pkl and actv file..\n",
      "--- 341.4046468734741 seconds ---\n",
      "Took 90.81229305267334 seconds to run for num_units 200.\n",
      "Loading pkl/4to20/network9_Relu2_epoch90_4to20.pkl and actv file..\n",
      "--- 346.01046895980835 seconds ---\n",
      "Took 96.5654981136322 seconds to run for num_units 200.\n",
      "All CSV files for net10 relu2 epoch90 already exist, skipping.\n"
     ]
    }
   ],
   "source": [
    "relus = list(range(2,3))\n",
    "epochs = np.arange(60, 91, 10)\n",
    "exps = np.arange(0, 10)\n",
    "#num_units = 100\n",
    "#rate_threshold = 0.05\n",
    "layer_numunits = {'relu1':290400, 'relu2':186624, 'relu3':64896, 'relu4':64896, 'relu5':43264}\n",
    "\n",
    "def svm_fit_with_fresh_units(exp, units_nonzero, actv, num_units):\n",
    "    units_sample = random.sample(units_nonzero, num_units)\n",
    "    return svm.SVM_fit_with_seed(exp, units_sample, actv)\n",
    "\n",
    "\n",
    "exps = range(10)\n",
    "for relu in relus:\n",
    "    for epoch in epochs:\n",
    "        for net in range(1, 11):\n",
    "            pkl_filename = f'pkl/4to20/network{net}_Relu{relu}_epoch{epoch}_4to20.pkl'\n",
    "\n",
    "            if not os.path.exists(pkl_filename):\n",
    "                print(f\"{pkl_filename} does not exist, skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Initialize missing_exps_dict as empty dictionary\n",
    "            missing_exps_dict = {}\n",
    "\n",
    "            for outer_num_units in np.arange(200, 201, 200):\n",
    "                # Define filenames before processing\n",
    "                csv_files = {exp: f'csv/svm_results/4to20/SVM prediction of He untrained net{net} relu{relu} epoch{epoch} {outer_num_units} nonzero activity units exp{exp} July2023_4to20.csv' for exp in exps}\n",
    "\n",
    "                # Filter out the experiments where CSV files already exist\n",
    "                missing_exps = [exp for exp, filename in csv_files.items() if not os.path.exists(filename)]\n",
    "\n",
    "                if missing_exps:\n",
    "                    missing_exps_dict[outer_num_units] = {exp: csv_files[exp] for exp in missing_exps}\n",
    "\n",
    "            if not missing_exps_dict:\n",
    "                print(f\"All CSV files for net{net} relu{relu} epoch{epoch} already exist, skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f'Loading {pkl_filename} and actv file..')\n",
    "            with open(pkl_filename, 'rb') as f:\n",
    "                units = pickle.load(f)\n",
    "\n",
    "            actv_net = actv_analysis.get_actv_net(net=net, relu=relu, epoch=epoch)\n",
    "            actv = actv_net.reshape(list(layer_numunits.values())[relu-1], 10, 10, 500)\n",
    "\n",
    "            # select units whose activities corresponding to images will be used for svm training and testing:\n",
    "            units_nonzero = [units[i].id for i in range(len(units)) if units[i].no_response_subset is not True]\n",
    "\n",
    "            for outer_num_units in np.arange(200, 201, 200):\n",
    "                if outer_num_units not in missing_exps_dict:\n",
    "                    print(f\"All CSV files for net{net} relu{relu} epoch{epoch} num_units {outer_num_units} already exist, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                units_sample = random.sample(units_nonzero, outer_num_units)\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Parallelize only the missing experiments\n",
    "                #y_preds = Parallel(n_jobs=-1)(delayed(svm.SVM_fit_shuffled_with_seed)(exp, units_sample, actv) for exp in missing_exps_dict[outer_num_units].keys())\n",
    "                # Parallelize only the missing experiments\n",
    "                y_preds = Parallel(n_jobs=-1)(delayed(svm_fit_with_fresh_units)(exp, units_nonzero, actv, outer_num_units) for exp in missing_exps_dict[outer_num_units].keys())\n",
    "\n",
    "\n",
    "                # Save results to CSV\n",
    "                for exp, y_pred in zip(missing_exps_dict[outer_num_units].keys(), y_preds):\n",
    "                    pd.Series(y_pred).to_csv(missing_exps_dict[outer_num_units][exp], index=True)\n",
    "\n",
    "                end_time = time.time()\n",
    "                print(f\"Took {end_time - start_time} seconds to run for num_units {outer_num_units}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836877d-eda7-4185-9ba8-da541fb74ddb",
   "metadata": {},
   "source": [
    "## Run SVM with specific types of units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c247646-da48-4db5-a657-3ce656334c60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relus = np.arange(4,5)\n",
    "epochs = np.arange(90, 91, 10)\n",
    "exps = np.arange(0, 10)\n",
    "#num_units = 100\n",
    "#rate_threshold = 0.05\n",
    "layer_numunits = {'relu1':290400, 'relu2':186624, 'relu3':64896, 'relu4':64896, 'relu5':43264}\n",
    "outer_num_units = 200\n",
    "\n",
    "exps = range(10)\n",
    "for relu in relus:\n",
    "    for epoch in epochs:\n",
    "        for net in range(1, 3):\n",
    "            pkl_filename = f'pkl/network{net}_Relu{relu}_epoch{epoch}.pkl'\n",
    "\n",
    "            if not os.path.exists(pkl_filename):\n",
    "                print(f\"{pkl_filename} does not exist, skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Initialize missing_exps_dict as empty dictionary\n",
    "            missing_exps_dict = {}\n",
    "\n",
    "            print(f'Loading {pkl_filename} and actv file..')\n",
    "            with open(pkl_filename, 'rb') as f:\n",
    "                units = pickle.load(f)\n",
    "\n",
    "            actv_net = actv_analysis.get_actv_net(net=net, relu=relu, epoch=epoch)\n",
    "            actv = actv_net.reshape(list(layer_numunits.values())[relu-1], 10, 10, 500)\n",
    "\n",
    "            # select units whose activities corresponding to images will be used for svm training and testing:\n",
    "            #units_nonzero = [units[i].id for i in range(len(units)) if units[i].no_response_subset is not True]\n",
    "            sn = [units[i].spearmanr_number for i in range(len(units))]\n",
    "            ss = [units[i].spearmanr_size for i in range(len(units))]\n",
    "            df_spearmanr = pd.DataFrame({'sn':sn, 'ss':ss})\n",
    "            # LNSS = df_spearmanr[(df_spearmanr['sn']>0.9) &( df_spearmanr['ss']<-0.9)].index\n",
    "            # LNLS = df_spearmanr[(df_spearmanr['sn']>0.9) &( df_spearmanr['ss']>0.9)].index\n",
    "            SNLS = df_spearmanr[(df_spearmanr['sn']<-0.9) &( df_spearmanr['ss']>0.9)].index\n",
    "            SNSS = df_spearmanr[(df_spearmanr['sn']<-0.9) &( df_spearmanr['ss']<-0.9)].index\n",
    "            type1 = SNLS; type2 = SNSS\n",
    "\n",
    "            for type1_prop in np.arange(0, 1.1, 0.1):\n",
    "                type2_prop = round(1-type1_prop, 1)\n",
    "\n",
    "                type1_sample = random.sample(list(type1), int(outer_num_units*type1_prop))\n",
    "                type2_sample = random.sample(list(type2), int(outer_num_units*type2_prop))\n",
    "                units_sample = np.union1d(type1_sample, type2_sample).astype(int)\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                y_preds = Parallel(n_jobs=-1)(delayed(svm.SVM_fit_with_seed)(exp, units_sample, actv) for exp in exps)\n",
    "\n",
    "                # Save results to CSV\n",
    "                for exp, y_pred in zip(exps, y_preds):\n",
    "                    csv_file = f'csv/svm_results/SVM_prediction_of_He_untrained_net{net}_relu{relu}_epoch{epoch}_{int(100*type1_prop)}_percent_SNLS_and_{int(100*type2_prop)}_percent_SNSS_units_exp{exp}_July2023.csv'\n",
    "                    df = pd.DataFrame({'y_pred': y_pred})\n",
    "                    df.to_csv(csv_file, index=False)\n",
    "\n",
    "                end_time = time.time()\n",
    "                print(f\"It took {end_time - start_time} seconds to run for num_units {outer_num_units}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6311e5-1fbf-4537-81e1-cf29b62949f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net=1; relu=4; epoch=90\n",
    "pkl_filename = f'pkl/network{net}_Relu{relu}_epoch{epoch}.pkl'\n",
    "with open(pkl_filename, 'rb') as f:\n",
    "    units = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f733e797-b704-4032-8c6f-f9bf2568267d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sn = [units[i].spearmanr_number for i in range(len(units))]\n",
    "ss = [units[i].spearmanr_size for i in range(len(units))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889667bf-749f-46b1-a766-41f1a8ae9257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.union1d(np.arange(0,1.1,0.1), np.arange(0,2.1,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9946be3-380d-427f-84b4-aeecb80f2be9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_spearmanr = pd.DataFrame(spearmanr = {'sn':sn, 'ss':ss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63831492-0d28-4fe4-90ef-a973fbe72278",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LNSS = df_spearmanr[(df_spearmanr['sn']>0.9) &( df_spearmanr['ss']<-0.9)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178df517-bd60-4129-aef2-a5ad5d6b3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=1; relu=5; epoch=90\n",
    "\n",
    "pkl_filename = f'pkl/network{net}_Relu{relu}_epoch{epoch}_4to20.pkl'\n",
    "\n",
    "with open(pkl_filename, 'rb') as f:\n",
    "    units = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48377029-32e3-4f5e-8410-e9db2d8f9da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.sum([units[i].PN == 4 for i in range(len(units))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26fabfb-cf55-4b28-b9ce-01aa413f5b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
