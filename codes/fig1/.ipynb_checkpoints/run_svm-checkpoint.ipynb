{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f43a57-8dde-48d7-9956-991c55b335fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "sys.path.append('../')\n",
    "\n",
    "from packages import actv_analysis, svm\n",
    "import seaborn as sns\n",
    "# from packages.svm import SVM_fit\n",
    "# from packages.load_csv import units_for_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd633cc2-a5de-404c-8f8e-4a101ff4a959",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nidx=range(0,10)\n",
    "sidx=range(3,10)\n",
    "img_inst=500\n",
    "\n",
    "for i in range(10):\n",
    "    rep_per_ns_combo=16\n",
    "    df_idx = svm.gen_SVM_input(nidx=nidx, sidx=sidx, rep_per_ns_combo=rep_per_ns_combo, img_inst=img_inst)\n",
    "    # save to csv file\n",
    "    df_idx.to_csv(f'svm_training_set{i}.csv', index=True)\n",
    "    \n",
    "    rep_per_ns_combo=4\n",
    "    df_idx = svm.gen_SVM_input(nidx=nidx, sidx=sidx, rep_per_ns_combo=rep_per_ns_combo, img_inst=img_inst)\n",
    "    # save to csv file\n",
    "    df_idx.to_csv(f'svm_test_set{i}.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9150d56a-9bc8-463d-920e-aed97a59ab5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Parameters\n",
    "#relus = range(2,6)\n",
    "relus = list(range(2,6))[::-1]\n",
    "epochs = np.arange(0, 91, 10)\n",
    "exps = np.arange(0, 10)\n",
    "num_units = 100\n",
    "rate_threshold = 0.05\n",
    "layer_numunits = {'relu1':290400, 'relu2':186624, 'relu3':64896, 'relu4':64896, 'relu5':43264}\n",
    "\n",
    "dir_path = os.path.dirname(os.path.realpath('../'))\n",
    "#path_for_units = f\"{dir_path}/dataframes/SVM/units/{num_units} units sampled from distribution higher than {rate_threshold} response rate including PN2 and PN20\"\n",
    "set_folder = f\"{dir_path}/dataframes/SVM\"  # retrieve training/test sets\n",
    "save_to_folder = f\"{dir_path}/dataframes/SVM_predictions\"\n",
    "\n",
    "for relu in relus:\n",
    "    for epoch in epochs:\n",
    "        for net in range(1, 2):\n",
    "            pkl_filename = f'pkl/network{net}_Relu{relu}_epoch{epoch}.pkl'\n",
    "            print(f'Loading {pkl_filename}..')\n",
    "            with open(pkl_filename, 'rb') as f:\n",
    "                units = pickle.load(f)\n",
    "            print(f'Loading actv ..')\n",
    "            actv_net = actv_analysis.get_actv_net(net=net, relu=relu, epoch=epoch)\n",
    "            actv = actv_net.reshape(list(layer_numunits.values())[relu-1], 10, 10, 500)\n",
    "            \n",
    "            # select units whose activities corresponding to images will be used for svm training and testing:\n",
    "            units_with_someresp = [units[i].id for i in range(len(units)) if units[i].response_rate_subset > rate_threshold]\n",
    "            \n",
    "            # Randomly choose \"num_units\" number of units from units_with_someresp without replacement\n",
    "            random.seed(exp)\n",
    "            units = random.sample(units_with_someresp, num_units)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            y_preds = Parallel(n_jobs=-1)(delayed(svm.SVM_fit)(units=units, actv=actv, exp=exp) for exp in exps)\n",
    "            end_time = time.time()\n",
    "            print(f\"Took {end_time - start_time} seconds to run.\")\n",
    "\n",
    "            [pd.Series(y_preds[exp]).to_csv(f'csv/SVM prediction of He untrained net{net} relu{relu} epoch{epoch} {num_units} units that are randomly drawn from distribution exp{exp} June2023.csv', index=True) for exp in exps]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79223798-f360-4899-896f-28eb49708f47",
   "metadata": {},
   "source": [
    "## Run SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb8f70-7498-4986-8beb-6b3bdffc8ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relus = list(range(2,6))[::-1]\n",
    "epochs = np.arange(0, 91, 10)\n",
    "exps = np.arange(0, 10)\n",
    "#num_units = 100\n",
    "#rate_threshold = 0.05\n",
    "layer_numunits = {'relu1':290400, 'relu2':186624, 'relu3':64896, 'relu4':64896, 'relu5':43264}\n",
    "\n",
    "exps = range(10)\n",
    "for relu in relus:\n",
    "    for epoch in epochs:\n",
    "        for net in range(1, 3):\n",
    "            pkl_filename = f'pkl/network{net}_Relu{relu}_epoch{epoch}.pkl'\n",
    "\n",
    "            if not os.path.exists(pkl_filename):\n",
    "                print(f\"{pkl_filename} does not exist, skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Initialize missing_exps_dict as empty dictionary\n",
    "            missing_exps_dict = {}\n",
    "\n",
    "            for outer_num_units in np.arange(200, 2001, 200):\n",
    "                # Define filenames before processing\n",
    "                csv_files = {exp: f'csv/svm_results/SVM prediction of He untrained net{net} relu{relu} epoch{epoch} {outer_num_units} nonzero activity units exp{exp} June2023.csv' for exp in exps}\n",
    "\n",
    "                # Filter out the experiments where CSV files already exist\n",
    "                missing_exps = [exp for exp, filename in csv_files.items() if not os.path.exists(filename)]\n",
    "\n",
    "                if missing_exps:\n",
    "                    missing_exps_dict[outer_num_units] = {exp: csv_files[exp] for exp in missing_exps}\n",
    "\n",
    "            if not missing_exps_dict:\n",
    "                print(f\"All CSV files for net{net} relu{relu} epoch{epoch} already exist, skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f'Loading {pkl_filename} and actv file..')\n",
    "            with open(pkl_filename, 'rb') as f:\n",
    "                units = pickle.load(f)\n",
    "\n",
    "            actv_net = actv_analysis.get_actv_net(net=net, relu=relu, epoch=epoch)\n",
    "            actv = actv_net.reshape(list(layer_numunits.values())[relu-1], 10, 10, 500)\n",
    "\n",
    "            # select units whose activities corresponding to images will be used for svm training and testing:\n",
    "            units_nonzero = [units[i].id for i in range(len(units)) if units[i].no_response_subset is not True]\n",
    "\n",
    "            for outer_num_units in np.arange(200, 2001, 200):\n",
    "                if outer_num_units not in missing_exps_dict:\n",
    "                    print(f\"All CSV files for net{net} relu{relu} epoch{epoch} num_units {outer_num_units} already exist, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                units_sample = random.sample(units_nonzero, outer_num_units)\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Parallelize only the missing experiments\n",
    "                y_preds = Parallel(n_jobs=-1)(delayed(svm.SVM_fit_with_seed)(exp, units_sample, actv) for exp in missing_exps_dict[outer_num_units].keys())\n",
    "\n",
    "                # Save results to CSV\n",
    "                for exp, y_pred in zip(missing_exps_dict[outer_num_units].keys(), y_preds):\n",
    "                    pd.Series(y_pred).to_csv(missing_exps_dict[outer_num_units][exp], index=True)\n",
    "\n",
    "                end_time = time.time()\n",
    "                print(f\"Took {end_time - start_time} seconds to run for num_units {outer_num_units}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836877d-eda7-4185-9ba8-da541fb74ddb",
   "metadata": {},
   "source": [
    "## Run SVM with specific types of units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c247646-da48-4db5-a657-3ce656334c60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pkl/network1_Relu4_epoch90.pkl and actv file..\n",
      "--- 100.83805799484253 seconds ---\n",
      "It took 52.76469302177429 seconds to run for num_units 200.\n",
      "It took 32.72128200531006 seconds to run for num_units 200.\n",
      "It took 32.43398571014404 seconds to run for num_units 200.\n",
      "It took 32.12636113166809 seconds to run for num_units 200.\n",
      "It took 31.03190779685974 seconds to run for num_units 200.\n",
      "It took 30.641894817352295 seconds to run for num_units 200.\n",
      "It took 32.14512276649475 seconds to run for num_units 200.\n",
      "It took 35.02132797241211 seconds to run for num_units 200.\n",
      "It took 34.48999810218811 seconds to run for num_units 200.\n",
      "It took 42.40022897720337 seconds to run for num_units 200.\n",
      "It took 58.83403491973877 seconds to run for num_units 200.\n",
      "Loading pkl/network2_Relu4_epoch90.pkl and actv file..\n",
      "--- 102.01696109771729 seconds ---\n",
      "It took 52.91724896430969 seconds to run for num_units 200.\n",
      "It took 37.628700256347656 seconds to run for num_units 200.\n",
      "It took 39.217652797698975 seconds to run for num_units 200.\n",
      "It took 33.81476020812988 seconds to run for num_units 200.\n",
      "It took 34.12379598617554 seconds to run for num_units 200.\n",
      "It took 35.584405183792114 seconds to run for num_units 200.\n",
      "It took 34.27181100845337 seconds to run for num_units 200.\n",
      "It took 35.50855302810669 seconds to run for num_units 200.\n",
      "It took 36.445425271987915 seconds to run for num_units 200.\n",
      "It took 40.26877188682556 seconds to run for num_units 200.\n",
      "It took 49.46380639076233 seconds to run for num_units 200.\n"
     ]
    }
   ],
   "source": [
    "relus = np.arange(4,5)\n",
    "epochs = np.arange(90, 91, 10)\n",
    "exps = np.arange(0, 10)\n",
    "#num_units = 100\n",
    "#rate_threshold = 0.05\n",
    "layer_numunits = {'relu1':290400, 'relu2':186624, 'relu3':64896, 'relu4':64896, 'relu5':43264}\n",
    "outer_num_units = 200\n",
    "\n",
    "exps = range(10)\n",
    "for relu in relus:\n",
    "    for epoch in epochs:\n",
    "        for net in range(1, 3):\n",
    "            pkl_filename = f'pkl/network{net}_Relu{relu}_epoch{epoch}.pkl'\n",
    "\n",
    "            if not os.path.exists(pkl_filename):\n",
    "                print(f\"{pkl_filename} does not exist, skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Initialize missing_exps_dict as empty dictionary\n",
    "            missing_exps_dict = {}\n",
    "\n",
    "            print(f'Loading {pkl_filename} and actv file..')\n",
    "            with open(pkl_filename, 'rb') as f:\n",
    "                units = pickle.load(f)\n",
    "\n",
    "            actv_net = actv_analysis.get_actv_net(net=net, relu=relu, epoch=epoch)\n",
    "            actv = actv_net.reshape(list(layer_numunits.values())[relu-1], 10, 10, 500)\n",
    "\n",
    "            # select units whose activities corresponding to images will be used for svm training and testing:\n",
    "            #units_nonzero = [units[i].id for i in range(len(units)) if units[i].no_response_subset is not True]\n",
    "            sn = [units[i].spearmanr_number for i in range(len(units))]\n",
    "            ss = [units[i].spearmanr_size for i in range(len(units))]\n",
    "            df_spearmanr = pd.DataFrame({'sn':sn, 'ss':ss})\n",
    "            LNSS = df_spearmanr[(df_spearmanr['sn']>0.9) &( df_spearmanr['ss']<-0.9)].index\n",
    "            LNLS = df_spearmanr[(df_spearmanr['sn']>0.9) &( df_spearmanr['ss']>0.9)].index\n",
    "\n",
    "            for LNSS_prop in np.arange(0, 1.1, 0.1):\n",
    "                LNLS_prop = round(1-LNSS_prop, 1)\n",
    "\n",
    "                LNSS_sample = random.sample(list(LNSS), int(outer_num_units*LNSS_prop))\n",
    "                LNLS_sample = random.sample(list(LNLS), int(outer_num_units*LNLS_prop))\n",
    "                units_sample = np.union1d(LNSS_sample, LNLS_sample).astype(int)\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                y_preds = Parallel(n_jobs=-1)(delayed(svm.SVM_fit_with_seed)(exp, units_sample, actv) for exp in exps)\n",
    "\n",
    "                # Save results to CSV\n",
    "                for exp, y_pred in zip(exps, y_preds):\n",
    "                    csv_file = f'csv/svm_results/SVM_prediction_of_He_untrained_net{net}_relu{relu}_epoch{epoch}_{int(100*LNSS_prop)}_percent_LNSS_and_{int(100*LNLS_prop)}_percent_LNLS_units_exp{exp}_July2023.csv'\n",
    "                    df = pd.DataFrame({'y_pred': y_pred})\n",
    "                    df.to_csv(csv_file, index=False)\n",
    "\n",
    "                end_time = time.time()\n",
    "                print(f\"It took {end_time - start_time} seconds to run for num_units {outer_num_units}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6311e5-1fbf-4537-81e1-cf29b62949f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net=1; relu=4; epoch=90\n",
    "pkl_filename = f'pkl/network{net}_Relu{relu}_epoch{epoch}.pkl'\n",
    "with open(pkl_filename, 'rb') as f:\n",
    "    units = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f733e797-b704-4032-8c6f-f9bf2568267d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sn = [units[i].spearmanr_number for i in range(len(units))]\n",
    "ss = [units[i].spearmanr_size for i in range(len(units))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "889667bf-749f-46b1-a766-41f1a8ae9257",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.2, 1.4,\n",
       "       1.6, 1.8, 2. ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.union1d(np.arange(0,1.1,0.1), np.arange(0,2.1,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9946be3-380d-427f-84b4-aeecb80f2be9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_spearmanr = pd.DataFrame(spearmanr = {'sn':sn, 'ss':ss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63831492-0d28-4fe4-90ef-a973fbe72278",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LNSS = df_spearmanr[(df_spearmanr['sn']>0.9) &( df_spearmanr['ss']<-0.9)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178df517-bd60-4129-aef2-a5ad5d6b3557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
